---
title: "ECO 395 Homework 2: Taylor Neal"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(rsample)
library(caret)
library(modelr)
library(glmnet)
library(lubridate)
library(ROCR)
library(foreach)
library(mosaic)
library(knitr)

capmetro <- read.csv("https://raw.githubusercontent.com/taylorneal/homework-2/master/data/capmetro_UT.csv", header = TRUE)
credit <- read.csv("https://raw.githubusercontent.com/taylorneal/homework-2/master/data/german_credit.csv", header = TRUE)
hotels_dev <- read.csv("https://raw.githubusercontent.com/taylorneal/homework-2/master/data/hotels_dev.csv", header = TRUE)
hotels_val <- read.csv("https://raw.githubusercontent.com/taylorneal/homework-2/master/data/hotels_val.csv", header = TRUE)

set.seed(9)
```
## 1) Data visualization: Capital Metro Data for UT


```{r capmetro-hourly, echo = FALSE}
capmetro = capmetro %>%
  mutate(day_of_week = factor(day_of_week, levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")), month = factor(month, levels = c("Sep", "Oct", "Nov")))

capmetro_hourly = capmetro %>% 
  group_by(hour_of_day, day_of_week, month) %>%
  summarise(mean_boarding = mean(boarding, na.rm=T), .groups = "drop")

ggplot(capmetro_hourly) + geom_line(aes(x = hour_of_day, y = mean_boarding, color = month)) + facet_wrap(~day_of_week) + labs(x = "Hour of Day", y = "Mean Number of Boardings", color = "Month") + theme_minimal() + ggtitle("Mean Hourly Capital Metro Boardings (to, from, and around UT)")

```

In the figure above, we consider mean number of Capital Metro boardings during each hour of the day separately for each day of the week. Each month in our data (September, October, and November) is plotted to analyze any changes occurring during this time frame. The hour of peak boardings (and the magnitude of peak boardings) remains quite consistent across all weekdays with only a slight drop off observed for Fridays. Weekends see far lower ridership. It is likely that the average boardings for Mondays in September are relatively lower because the Labor Day holiday is being averaged in as one of only four Mondays during that month in 2018. And we would expect that ridership would be lower on holidays given the decreased reliance on the metro system for commuting. Similarly, the November lines for Wednesday, Thursday, and Friday are likely lower because of the Thanksgiving holiday averaging in amongst the typical weekdays. Overall, this figure seems to indicate that mean boardings across typical workdays are remarkably consistent based on this three month time frame.

```{r boardings-vs-temp, echo = FALSE}

ggplot(capmetro) + geom_point(aes(x = temperature, y = boarding, color = weekend)) + facet_wrap(~hour_of_day) + labs(x = "Temperature (degrees F)", y = "Boardings", color = "Weekend / Weekday") + theme_minimal() + ggtitle("Boardings vs Temperature, Faceted by Hour of the Day")
```

Each dot in the figure above represents number of boardings and temperature during a 15min increment in our Sept.-Nov. 2018 Capital Metro data. The data are faceted by hour of the day in order to compare similar periods of any given day with differing temperatures to explore temperature's causal impact on boardings. Based on the resulting figure, it does not appear that temperature has a noticeable impact on UT student ridership when accounting for hour of the day and whether or not it is a weekday / weekend. 

## 2) Saratoga House Prices

In this section, we seek to determine whether a linear or k-nearest neighbors  approach is better suited to modeling Saratoga housing prices for determining appropriate property tax assessments. Our data consists of 15 house characteristics in addition to sales price. We want to ensure a flexible  approach is taken such that we determine the optimal usage of available data in each modeling case. Additionally, when evaluating our model results we will seek to reduce randomness associated with train / test splits of the data by cycling through 20 random folds of said splits (where 20% of the data is reserved for testing in each fold).

#### Linear Regression Model

```{r echo = FALSE}
data(SaratogaHouses)

saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)

lm_start = lm(price ~ lotSize + landValue + age + livingArea + pctCollege + bedrooms + bathrooms + fuel + centralAir + waterfront + newConstruction, data = saratoga_train)

lm_step = step(lm_start, scope = ~(.)^2, trace = 0)

SH_lm_folds = crossv_mc(SaratogaHouses, n = 20, test = 0.2)

mean_lm_rmse <- map(SH_lm_folds$train, ~ lm(lm_step$terms, data = .)) %>% 
  map2_dbl(SH_lm_folds$test, modelr::rmse) %>% mean()

```

For the linear model, we utilized stepwise selection to choose our model components and interaction variables. After a bit of trial and error choosing an appropriate starting model for the stepwise process, we settled on utilizing the following variables: lotSize, landValue, age, livingArea, pctCollege, bedrooms, bathrooms, fuel, centralAir, waterfront, newConstruction. From this starting point, the end result stepwise model was relatively stable when varying the split used to arrive at a more robust solution with interaction variables and no excluded individual variables from the dataset got added in. After settling on the linear model to use (with the additional interaction terms arrived at via the stepwise process), we utilized 20 random folds of the housing sales price data and determined an average RMSE across the folds of approximately 60,146.

#### K-nearest-neighbor Regression Model

```{r echo = FALSE}
SaratogaHouses <- SaratogaHouses %>% mutate(across(lotSize:rooms, scale))

SH_knn_folds = crossv_mc(SaratogaHouses, n = 20, test = 0.2)

k_grid <- c(seq(3, 29, by = 2))

my_knn_grid <- foreach(k = k_grid, .combine = rbind) %do% {
  mean_knn_rmse <- map(SH_knn_folds$train, ~ knnreg(price ~ . - fireplaces - rooms - heating - sewer, k = k, data = .)) %>% 
    map2_dbl(SH_knn_folds$test, modelr::rmse) %>% mean()
  c(k = k, rmse = mean_knn_rmse)
} %>% as.data.frame %>% arrange(rmse)

best_knn <- my_knn_grid[1,]

```

As a first step for K-nearest neighbors, we standardized the non-factor predictor variables in our data to account for the large differences in scale across the different variables. The K-nearest-neighbors approach required us to try various values of k for each of the 20 random folds of our train / test splits. We allowed k to vary from 3 to 29 (skipping even numbers to avoid any tie issues). K was chosen based on lowest RMSE across the 20 folds. The resulting K and mean RMSE were 9 and approximately 61,967 respectively. 

#### Conclusions

Through this exercise we discovered that not only did the mean RMSE across folds come out slightly lower for the linear model, but the linear stepwise model was also more stable as we allowed for the randomness of completely different train / test folds. Further work could be done extending (and likely improving) the linear model by using the Lasso technique to determine appropriate interaction terms. But as an initial recommendation, it appears that a linear model is advantageous for this modeling problem when compared to K-nearest-neighbors.

## 3) Classification and Retrospective Sampling

```{r bar-credit-history, echo = FALSE}
credit_history = credit %>%
  group_by(history) %>% 
  summarize(default_rate = mean(Default))
credit_history$history = str_to_title(credit_history$history)

ggplot(credit_history, aes(y = default_rate, x = history)) + geom_bar(stat = "identity", fill = "steelblue") + xlab("Credit History") + ylab("Default Rate") + theme_minimal() + ggtitle("Default Probability by Credit History")

```

In the figure above, showing a bar plot of the proportion of defaults in our dataset for the different credit history classifications, we find a counter intuitive visualization of our data. It appears that a much higher proportion of the better credit history classifications (i.e., "Good" and "Poor" when compared with "Terrible") were defaults. This is likely due to the nature of how this data was collected. This "case-control" design took defaults and attempted to match them with similar cases where the other case did not default. But this oversampling of defaults and unnatural sampling combination arising from seeking out the most similar cases with different results has lead to the creation of an odd dataset that does not closely resemble any real world population of cases. Thus, even before attempting to model default likelihood, we know that it is unlikely a useful model for predicting out of sample default will be possible.
 
```{r logit-coefficients, echo = FALSE}
logit_default = glm(Default ~ duration + amount + installment + age + history + purpose + foreign, data = credit, family = "binomial")

logit_default %>% 
  coef() %>% 
  round(2) %>% 
  kable(col.names = c("Coefficient"))

```

The coefficients obtained in the table above from fitting a simple logistic regression model of default on duration, amount, installment, age, history, purpose, and foreign confirm our fears based on the data collection process and earlier plot. We see that having a worse credit history rating ("Poor" or "Terrible" - given their negative coefficients) is actually lowering default probability in this derived model. This is clearly problematic and an unrealistic thing to expect when trying to apply such a model to screen prospective borrowers. But this is very likely due to the sampling process for how this data has been gathered. And, given that the goal is likely to use prior default data to avoid taking on risky loans in the future, changes to the bank's sampling scheme should be recommended. It would be much better to have a sample set with default rates more representative of prospective borrowers as a whole where defaulted loans are not vastly over represented. 

## 4) Children and Hotel Reservations

For this modeling problem we added two engineered features (a dummy variable for two adults and arrival month as a factor). Utilizing these additional variables along with an interaction term between reserved_room_type and the two adults dummy, we were able to gain performance on both baselines in terms of RMSE and the area under the ROC curve. Please see model verification steps below (utilizing data that was completely held out of the fitting and testing phases).

```{r ROC-curve, echo = FALSE}

folds <- 5

fold_id <- rep(1:folds, length = nrow(hotels_dev)) %>% sample
baseline_1 <- foreach(fold = 1:5, .combine = "c") %do% {
  train_ <- hotels_dev[fold_id!=fold,]
  test_ <- hotels_dev[fold_id==fold,]
  baseline_1_model <- glm(children ~ market_segment + adults + customer_type +
                            is_repeated_guest, family = "binomial", 
                          data = train_)
  pred <- predict(baseline_1_model, test_, type="response")
  pred <- prediction(pred, test_$children)
  perf <- performance(pred, "auc")
  perf@y.values[[1]]
} %>% mean()

fold_id <- rep(1:folds, length = nrow(hotels_dev)) %>% sample
baseline_2 <- foreach(fold = 1:5, .combine = "c") %do% {
  train_ <- hotels_dev[fold_id!=fold,]
  test_ <- hotels_dev[fold_id==fold,]
  baseline_2_model <- glm(children ~ . - arrival_date, family = "binomial", 
                          data = train_)
  pred <- predict(baseline_2_model, test_, type="response")
  pred <- prediction(pred, test_$children)
  perf <- performance(pred, "auc")
  perf@y.values[[1]]
} %>% mean()

hotels_dev = hotels_dev %>% 
  mutate(two_adults = adults == 2,month_arrive = as.factor(month(as.Date(arrival_date))))
hotels_val = hotels_val %>% 
  mutate(two_adults = adults == 2,month_arrive = as.factor(month(as.Date(arrival_date))))

fold_id <- rep(1:folds, length = nrow(hotels_dev)) %>% sample
my_attempt <- foreach(fold = 1:5, .combine = "c") %do% {
  train_ <- hotels_dev[fold_id!=fold,]
  test_ <- hotels_dev[fold_id==fold,]
  my_model <- glm(children ~ . - arrival_date - deposit_type -
                    previous_cancellations + two_adults:reserved_room_type,
                  family = "binomial", 
                          data = train_)
  pred <- predict(my_model, test_, type="response")
  pred <- prediction(pred, test_$children)
  perf <- performance(pred, "auc")
  perf@y.values[[1]]
} %>% mean()

pred <- predict(my_model, newdata = hotels_val, type = "response")
pred <- prediction(pred, hotels_val$children)
ROC <- performance(pred, "tpr", "fpr")
plot(ROC, print.auc = TRUE)

```

The plot above shows the ROC for our final model with predictions compared to the validation set of data which was held out of this analysis until this step.

```{r val-accuracy, echo = FALSE}
folds = 20
fold_id = rep(1:folds, length = nrow(hotels_val$children)) %>% sample
accuracy <- foreach(fold = 1:20, .combine = "c") %do% {
  val_step <- hotels_val[fold_id == fold,]
  children <- hotels_val$children[fold_id == fold]
  
  pred <- predict(my_model, newdata = val_step, type = "response")
  
  pred <- data.frame(y = children, y_pred = if_else(pred > 0.5, 1, 0))
  
  (xtabs(~ ., pred) %>% diag() %>% sum()) / nrow(pred)
  
}
table_ = data.frame(Trial = rep(1:20), Accuracy = accuracy %>% round(4)*100)
   
kable(table_)

```
As seen in the table above, the model does a decent job at predicting the number of children with most accuracy rates over 90%.